{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "import sys\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def load_data(input_file):\n",
    "    \"\"\"Load JSON data\"\"\"\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        if not isinstance(data, list):\n",
    "            print(\"The JSON data is not in a list format. Please ensure the input file is a JSON array.\")\n",
    "            sys.exit(1)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{input_file}' not found.\")\n",
    "        sys.exit(1)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error in JSON format. Please check the input file format.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def save_data(data, output_file):\n",
    "    \"\"\"Save JSON data\"\"\"\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving the file: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def contains_keywords(text, keywords):\n",
    "    \"\"\"Check if the text contains any of the keywords (case-sensitive)\"\"\"\n",
    "    for kw in keywords:\n",
    "        if kw in text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def remove_entries_with_keywords(data, keywords):\n",
    "    \"\"\"Remove Q&A entries that contain specific keywords\"\"\"\n",
    "    filtered_data = []\n",
    "    deleted_count = 0\n",
    "    for entry in data:\n",
    "        instruction = entry.get('instruction', '')\n",
    "        output = entry.get('output', '')\n",
    "        if contains_keywords(instruction, keywords) or contains_keywords(output, keywords):\n",
    "            deleted_count += 1\n",
    "        else:\n",
    "            filtered_data.append(entry)\n",
    "    return filtered_data, deleted_count\n",
    "\n",
    "def remove_duplicate_questions(data):\n",
    "    \"\"\"\n",
    "    Remove duplicate Q&A entries (based on exact match of the 'instruction' field).\n",
    "    Use hash values for quick detection of duplicates.\n",
    "    \"\"\"\n",
    "    filtered_data = []\n",
    "    seen_hashes = set()\n",
    "    deleted_count = 0\n",
    "    for entry in data:\n",
    "        instruction = entry.get('instruction', '')\n",
    "        # Generate MD5 hash of the instruction\n",
    "        instruction_hash = hashlib.md5(instruction.encode('utf-8')).hexdigest()\n",
    "        if instruction_hash in seen_hashes:\n",
    "            deleted_count += 1\n",
    "            continue\n",
    "        seen_hashes.add(instruction_hash)\n",
    "        filtered_data.append(entry)\n",
    "    return filtered_data, deleted_count\n",
    "\n",
    "def remove_short_qnas(data, min_question_length=10, min_answer_length=20):\n",
    "    \"\"\"Remove Q&A entries with too short questions or answers\"\"\"\n",
    "    filtered_data = []\n",
    "    deleted_count = 0\n",
    "    for entry in data:\n",
    "        instruction = entry.get('instruction', '')\n",
    "        output = entry.get('output', '')\n",
    "        if len(instruction) < min_question_length or len(output) < min_answer_length:\n",
    "            deleted_count += 1\n",
    "        else:\n",
    "            filtered_data.append(entry)\n",
    "    return filtered_data, deleted_count\n",
    "\n",
    "def remove_similar_questions(data, similarity_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Use sentence embeddings and cosine similarity to remove similar Q&A entries.\n",
    "    \"\"\"\n",
    "    instructions = [entry.get('instruction', '') for entry in data]\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')  # Choose an appropriate pre-trained model\n",
    "    embeddings = model.encode(instructions, convert_to_tensor=False)\n",
    "    \n",
    "    cosine_sim = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Get indices of the upper triangle matrix to avoid duplicate calculations\n",
    "    upper_tri = np.triu_indices(len(cosine_sim), k=1)\n",
    "    similar_pairs = np.where(cosine_sim[upper_tri] > similarity_threshold)\n",
    "    \n",
    "    # Check if similar_pairs contains two arrays\n",
    "    if len(similar_pairs) < 2:\n",
    "        similar_pairs = ([], [])\n",
    "    \n",
    "    to_delete = set()\n",
    "    for i, j in zip(*similar_pairs):\n",
    "        idx1 = i\n",
    "        idx2 = j\n",
    "        # Only delete the later occurrence, keeping the first one\n",
    "        to_delete.add(idx2 + 1)\n",
    "    \n",
    "    filtered_data = [entry for idx, entry in enumerate(data) if idx not in to_delete]\n",
    "    deleted_count = len(to_delete)\n",
    "    return filtered_data, deleted_count\n",
    "\n",
    "def main():\n",
    "    # Configuration parameters\n",
    "    input_file = 'aggregated_qna.json'          # Input file name\n",
    "    output_file = 'cleaned_qna.json'       # Output file name\n",
    "    keywords = [\n",
    "        'It seems that',\n",
    "        'in the study',\n",
    "        'It seems',\n",
    "        'To provide a more comprehensive answer',\n",
    "        'It appears that',\n",
    "        'main findings'\n",
    "    ]  # List of keywords to filter, case-sensitive\n",
    "    # min_question_length = 10               # Minimum length of the question (characters)\n",
    "    # min_answer_length = 20                 # Minimum length of the answer (characters)\n",
    "    similarity_threshold = 0.8              # Similarity threshold\n",
    "    \n",
    "    # Step 1: Load data\n",
    "    data = load_data(input_file)\n",
    "    original_count = len(data)\n",
    "    \n",
    "    # Step 2: Remove entries with keywords\n",
    "    data, deleted_keywords = remove_entries_with_keywords(data, keywords)\n",
    "    \n",
    "    # Step 3: Remove duplicate Q&A entries\n",
    "    data, deleted_duplicates = remove_duplicate_questions(data)\n",
    "    \n",
    "    # # Step 4: Remove short Q&A entries\n",
    "    # data, deleted_short = remove_short_qnas(data, min_question_length, min_answer_length)\n",
    "    \n",
    "    # Step 5: Remove similar Q&A entries\n",
    "    data, deleted_similar = remove_similar_questions(data, similarity_threshold)\n",
    "    \n",
    "    # Count the retained records\n",
    "    retained_count = len(data)\n",
    "    total_deleted = deleted_keywords + deleted_duplicates + deleted_short + deleted_similar\n",
    "    \n",
    "    # Step 6: Save the cleaned data\n",
    "    save_data(data, output_file)\n",
    "    \n",
    "    # Output statistics\n",
    "    print(f\"Original record count: {original_count} entries\")\n",
    "    print(f\"Deleted records containing keywords: {deleted_keywords} entries\")\n",
    "    print(f\"Deleted duplicate records: {deleted_duplicates} entries\")\n",
    "    # print(f\"Deleted short records: {deleted_short} entries\")\n",
    "    print(f\"Deleted similar records: {deleted_similar} entries\")\n",
    "    print(f\"Total deleted records: {total_deleted} entries\")\n",
    "    print(f\"Retained records: {retained_count} entries\")\n",
    "    print(f\"Cleaned data has been saved to '{output_file}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
