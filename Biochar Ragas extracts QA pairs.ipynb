{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c999342f6af42fea05f8be502a1b95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f91a7860e37477eba4e5f37b58c2737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0340d7741fda4ea1a38f00b4d312d727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb28e000ff645e7b0ba5b326dc8ec93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddee31ae78c24dc2a1540ef2fad6e1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03513fea21e64ebb91d7999e9fcdbef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有问答对已保存为 c:\\Users\\yu429\\Desktop\\all_qa_pairs.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pdfplumber\n",
    "from openai import OpenAI\n",
    "from ragas.testset.docstore import Document\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.llms.prompt import Prompt\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas.llms.base import LangchainLLMWrapper\n",
    "import time\n",
    "\n",
    "# Define the folder path where the PDF files are located\n",
    "pdf_folder_path = \"\"\n",
    "\n",
    "# Set the OpenAI API key\n",
    "client = OpenAI(\n",
    "    base_url=\"\",\n",
    "    api_key=\"\"\n",
    ")\n",
    "\n",
    "# Initialize the generator and embedding model\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4o\", api_key=\"\", base_url=\"\")\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o\", api_key=\"\", base_url=\"\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=\"\", base_url=\"\")\n",
    "\n",
    "# Wrap the model\n",
    "llm_wrapper = LangchainLLMWrapper(generator_llm)\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "# Define the prompt for question generation\n",
    "qa_prompt = Prompt(\n",
    "    name=\"question_generation\",\n",
    "    instruction=\"Please read the following text about anaerobic digestion and extract any relevant information related to the anaerobic digestion process, including but not limited to organic material resources and raw material sources, pre-treatment methods, preparation equipment, modification methods, or key parameters such as treatment temperature, heating rate, and residence time. Additionally, collect any detailed information on biogas yield, methane content, and the composition and properties of the digestate, including nutrient content, residual volatile solids after digestion, pH, and particle size. Based on the extracted information, context, and relevant knowledge, generate a question related to either the anaerobic digestion process or biogas yield and methane content.\",\n",
    "    examples=[\n",
    "        {\n",
    "            \"answer\": \"The pretreatment agent significantly affects methane production during anaerobic digestion. For instance, NaOH pretreatment can enhance methane production efficiency by disrupting lignocellulosic structures, thereby increasing the availability of degradable substrates. However, if the concentration of NaOH is too high, it may lead to the accumulation of inhibitory substances like sodium ions, which can hinder microbial activity. Additionally, lignin content in the feedstock plays a crucial role, with higher lignin levels generally leading to lower methane yields due to lignin's resistance to biodegradation.\",\n",
    "            \"context\": \"The study demonstrated that different chemical pretreatments, such as using NaOH, KOH, and alkaline hydrogen peroxide (AHP), have varying impacts on the methane yield from lignocellulosic waste. It was observed that lignin content above 15% in the raw material significantly reduces methane production, even after chemical pretreatment. The pretreatment conditions, including the concentration of chemicals and the digestion time, were identified as key factors influencing the methane yield.\",\n",
    "            \"output\": {\"question\": \"How do different chemical pretreatments and lignin content in lignocellulosic feedstock affect methane production in anaerobic digestion, and what are the optimal conditions for maximizing methane yield?\"}\n",
    "        },\n",
    "        {\n",
    "            \"answer\": \"Digestion time (DT) is a critical factor affecting methane yield in anaerobic digestion. Studies have shown that extending digestion time enhances methane production, especially during the first 18 days when microbial metabolism and the conversion of organic matter are most active. However, beyond 18 days, the rate of increase in methane yield tends to slow down, indicating that methane production reaches saturation at this stage. Additionally, the choice of pretreatment agents, such as NaOH, KOH, and AHP, significantly impacts methane yield, with NaOH and AHP showing better performance in enhancing methane yield with extended digestion time.\",\n",
    "            \"context\": \"Research indicates that digestion time and the choice of pretreatment agents are crucial for optimizing methane yield in anaerobic digestion. Experimental data analysis revealed that methane yield increases significantly up to 18 days of digestion time, after which the increase rate levels off. Different pretreatment agents vary in effectiveness, with NaOH and AHP showing significant improvement in methane yield during extended digestion times.\",\n",
    "            \"output\": {\"question\": \"How does digestion time affect methane yield in anaerobic digestion? How can the use of different pretreatment agents (such as NaOH, KOH, and AHP) be combined with digestion time to optimize methane production?\"}\n",
    "        }\n",
    "    ],\n",
    "    input_keys=[\"answer\", \"context\"],\n",
    "    output_key=\"output\",\n",
    "    output_type=\"json\"\n",
    ")\n",
    "\n",
    "# Get the current working directory\n",
    "current_folder_path = os.getcwd()\n",
    "\n",
    "# Define the path for the output JSON file\n",
    "output_json_path = os.path.join(current_folder_path, \"all_qa_pairs.json\")\n",
    "\n",
    "# Define the path for the progress file\n",
    "progress_file_path = os.path.join(current_folder_path, \"progress.txt\")\n",
    "\n",
    "# Path for the error log file\n",
    "error_log_path = os.path.join(current_folder_path, \"error_log.txt\")\n",
    "\n",
    "# Define the maximum retry attempts\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "def save_progress(filename):\n",
    "    \"\"\"Save the filename of the last processed file\"\"\"\n",
    "    with open(progress_file_path, 'w') as f:\n",
    "        f.write(filename)\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Load the filename of the last processed file\"\"\"\n",
    "    if os.path.exists(progress_file_path):\n",
    "        with open(progress_file_path, 'r') as f:\n",
    "            return f.read().strip()\n",
    "    return None\n",
    "\n",
    "def log_error(filename, error):\n",
    "    \"\"\"Log the files that failed to process\"\"\"\n",
    "    with open(error_log_path, 'a') as f:\n",
    "        f.write(f\"Failed to process {filename}: {error}\\n\")\n",
    "\n",
    "def append_to_json(file_path, new_data):\n",
    "    # If the file does not exist, create a new file and write the data\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump([new_data], f, ensure_ascii=False, indent=4)\n",
    "    else:\n",
    "        # If the file exists, load old data and append new data\n",
    "        with open(file_path, 'r+', encoding='utf-8') as f:\n",
    "            try:\n",
    "                # Load old data\n",
    "                old_data = json.load(f)\n",
    "                old_data.append(new_data)\n",
    "                # Move the cursor to the beginning of the file\n",
    "                f.seek(0)\n",
    "                f.truncate()\n",
    "                json.dump(old_data, f, ensure_ascii=False, indent=4)\n",
    "            except json.JSONDecodeError:\n",
    "                # If the file is empty or there is a format issue\n",
    "                json.dump([new_data], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Function to extract content between Introduction and References\n",
    "def extract_relevant_content(text):\n",
    "    start_keywords = [\"Introduction\", \"INTRODUCTION\"]\n",
    "    end_keywords = [\"References\", \"REFERENCES\", \"Bibliography\"]\n",
    "\n",
    "    start_index = None\n",
    "    end_index = None\n",
    "\n",
    "    # Find the start position of the Introduction\n",
    "    for keyword in start_keywords:\n",
    "        start_index = text.find(keyword)\n",
    "        if start_index != -1:\n",
    "            break\n",
    "\n",
    "    # Find the end position of the References\n",
    "    for keyword in end_keywords:\n",
    "        end_index = text.find(keyword)\n",
    "        if end_index != -1:\n",
    "            break\n",
    "\n",
    "    # Extract the relevant content\n",
    "    if start_index != -1 and end_index != -1 and start_index < end_index:\n",
    "        return text[start_index:end_index]\n",
    "    elif start_index != -1:\n",
    "        return text[start_index:]\n",
    "    elif end_index != -1:\n",
    "        return text[:end_index]\n",
    "    else:\n",
    "        return None  # Return None if no matches are found\n",
    "\n",
    "# Load the progress of the last processed file\n",
    "last_processed_file = load_progress()\n",
    "start_processing = False if last_processed_file else True\n",
    "\n",
    "# Iterate over all PDF files in the folder\n",
    "for filename in os.listdir(pdf_folder_path):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        # Skip already processed files based on the progress record\n",
    "        if not start_processing:\n",
    "            if filename == last_processed_file:\n",
    "                start_processing = True\n",
    "            continue\n",
    "\n",
    "        pdf_path = os.path.join(pdf_folder_path, filename)\n",
    "        retries = 0\n",
    "\n",
    "        while retries < MAX_RETRIES:\n",
    "            try:\n",
    "                # Extract text from the PDF\n",
    "                with pdfplumber.open(pdf_path) as pdf:\n",
    "                    text = ''.join(page.extract_text() for page in pdf.pages if page.extract_text())\n",
    "\n",
    "                # Extract content between Introduction and References\n",
    "                relevant_text = extract_relevant_content(text)\n",
    "\n",
    "                if relevant_text:  # If valid content is extracted\n",
    "                    # Convert the extracted text into a Document object\n",
    "                    document = Document(\n",
    "                        page_content=relevant_text,\n",
    "                        metadata={\"filename\": filename}\n",
    "                    )\n",
    "\n",
    "                    # Generate the test set using Langchain's model without passing the Prompt directly\n",
    "                    testset = generator.generate_with_langchain_docs(\n",
    "                                               [document],\n",
    "                        test_size=10\n",
    "                    )\n",
    "\n",
    "                    # Manually generate Q&A pairs using the Prompt\n",
    "                    for qa in testset.to_pandas().itertuples():\n",
    "                        # Call OpenAI API to expand the answer\n",
    "                        completion = client.chat.completions.create(\n",
    "                            model=\"gpt-4o\",\n",
    "                            messages=[\n",
    "                                {\"role\": \"system\", \"content\": \"You're an expert on anaerobic digestion.\"},\n",
    "                                {\"role\": \"user\", \"content\": f\"Based on the given answer and your knowledge, provide a more comprehensive answer.\\n\\nAnswer: {qa.ground_truth}\"}\n",
    "                            ],\n",
    "                            max_tokens=4096\n",
    "                        )\n",
    "                        full_answer = completion.choices[0].message.content\n",
    "\n",
    "                        new_entry = {\n",
    "                            \"instruction\": qa.question,\n",
    "                            \"input\": \"\",\n",
    "                            \"output\": full_answer\n",
    "                        }\n",
    "\n",
    "                        # Save the generated Q&A pair to the JSON file after processing each PDF\n",
    "                        append_to_json(output_json_path, new_entry)\n",
    "\n",
    "                    # Save the progress after processing each PDF\n",
    "                    save_progress(filename)\n",
    "                    break  # Exit retry loop after successful processing\n",
    "\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Error processing file {filename}: {str(e)}. Retry attempt {retries}.\")\n",
    "                if retries >= MAX_RETRIES:\n",
    "                    print(f\"File {filename} reached maximum retry attempts, skipping this file.\")\n",
    "                    log_error(filename, str(e))\n",
    "                    break\n",
    "                time.sleep(10)  # Wait for 10 seconds before retrying\n",
    "\n",
    "print(f\"All Q&A pairs have been saved to {output_json_path}\")\n",
    "\n",
    "                       \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
